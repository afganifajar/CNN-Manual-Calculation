{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Manual_Calculation_PyTorch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MOd_ECD0vEkX"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = torch.Tensor([[[[ 1, -1,  0,  0, -1],\n",
        "                       [ 0,  1, -1,  1,  1],\n",
        "                       [ 1, -1,  1,  0, -1],\n",
        "                       [ 1,  1,  0, -1,  0],\n",
        "                       [-1,  0,  0,  1,  0]]]])\n",
        "target_onehot = torch.Tensor([[1, 0, 0]])\n",
        "target = torch.tensor([0])\n",
        "\n",
        "conv1_weight = torch.Tensor([[[[ 1, -1,  2],\n",
        "                               [-1, -2,  0],\n",
        "                               [ 0,  2, -1]]],\n",
        "                             [[[-2,  1, -1],\n",
        "                               [ 2,  0,  0],\n",
        "                               [ 0,  1, -1]]]])\n",
        "\n",
        "linear1_weight = torch.Tensor([[2, 0, 0, 2, 1, -2, -1, 0, 0, -1, 1, -3, -1, -3, -2, -1, 2, -3],\n",
        "                               [-2, 3, 2, -2, -3, 0, 2, 2, 1, 3, -3, -3, 0, 1, 3, 2, 3, -2],\n",
        "                               [-3, 3, 0, 1, -3, 1, 2, 3, -2, 2, 1, 0, 3, 0, -3, -3, -1, -3],\n",
        "                               [-2, -1, 2, 0, -2, 0, 0, 2, -3, 3, 3, 2, 3, -3, -3, 0, -1, 0]])\n",
        "\n",
        "linear2_weight = torch.Tensor([[0, 0, -1, 1],\n",
        "                               [0, 1, 1, -3],\n",
        "                               [1, -2, -3, 3]])\n",
        "\n",
        "linear1_bias = torch.Tensor([[2, -1, 4, 0]])\n",
        "\n",
        "linear2_bias = torch.Tensor([[3, -1, 2]])\n",
        "\n",
        "conv = relu1 = flatten = linear1 = relu2 = linear2 = softmax = 0"
      ],
      "metadata": {
        "id": "ywePpneUvJ2U"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(MyNet, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(1, 2, kernel_size=3, bias=False)\n",
        "    self.linear1 = nn.Linear(18, 4)\n",
        "    self.linear2 = nn.Linear(4, 3)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.softmax = nn.Softmax()\n",
        "    \n",
        "    self.conv1.weight = nn.Parameter(conv1_weight)\n",
        "    self.linear1.weight = nn.Parameter(linear1_weight)\n",
        "    self.linear1.bias = nn.Parameter(linear1_bias)\n",
        "    self.linear2.weight = nn.Parameter(linear2_weight)\n",
        "    self.linear2.bias = nn.Parameter(linear2_bias)\n",
        "  \n",
        "  def forward(self, data):\n",
        "    conv    = self.conv1(data)\n",
        "    relu1   = self.relu(conv)\n",
        "    flatten = relu1.view(relu1.size(0), -1) # Flatten\n",
        "    linear1 = self.linear1(flatten)\n",
        "    relu2   = self.relu(linear1)\n",
        "    linear2 = self.linear2(relu2)\n",
        "    softmax = self.softmax(linear2)\n",
        "    return softmax"
      ],
      "metadata": {
        "id": "jpvxLrAgvM_O"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyNet()\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "result = model(data)\n",
        "loss = criterion(torch.log(result), target)\n",
        "\n",
        "print('Forward Propagation : ')\n",
        "print('convolutional output :\\n', conv)\n",
        "print('relu activation output :\\n', relu1)\n",
        "print('flatten result :\\n', flatten)\n",
        "print('linear1 output :\\n', linear1)\n",
        "print('relu activation output :\\n', relu2)\n",
        "print('linear2 output :\\n', linear2)\n",
        "print('softmax activation output:\\n', softmax)\n",
        "\n",
        "print('\\Loss Calculation')\n",
        "print('result = ', result)\n",
        "print('target = ', target)\n",
        "print(\"loss = \",   loss)\n",
        "\n",
        "print('\\nBackward Propagation')\n",
        "print('old linear 2 weight = ', linear2_weight)\n",
        "print('old linear 1 weight = ', linear1_weight)\n",
        "print('old convolution weight = ', conv1_weight)\n",
        "\n",
        "optimizer.zero_grad()\n",
        "loss.backward()\n",
        "optimizer.step()\n",
        "\n",
        "print('\\nnew linear 2 grad = ', model.linear2.weight.grad)\n",
        "print('new linear 2 bias grad = ', model.linear2.bias.grad)\n",
        "print('new linear 1 grad = ', model.linear1.weight.grad)\n",
        "print('new linear 1 bias grad = ', model.linear1.bias.grad)\n",
        "print('new convolution grad = ', model.conv1.weight.grad)\n",
        "\n",
        "print('\\nnew linear 2 weight = ', model.linear2.weight)\n",
        "print('new linear 2 bias = ', model.linear2.bias)\n",
        "print('new linear 1 weight = ', model.linear1.weight)\n",
        "print('new linear 1 bias = ', model.linear1.bias)\n",
        "print('new convolution weight = ', model.conv1.weight)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYImoubmxICX",
        "outputId": "34811175-cef5-40f6-cf0d-d01ae8d4d535"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forward Propagation : \n",
            "convolutional output :\n",
            " 0\n",
            "relu activation output :\n",
            " 0\n",
            "flatten result :\n",
            " 0\n",
            "linear1 output :\n",
            " 0\n",
            "relu activation output :\n",
            " 0\n",
            "linear2 output :\n",
            " 0\n",
            "softmax activation output:\n",
            " 0\n",
            "\\Loss Calculation\n",
            "result =  tensor([[0.1189, 0.8789, 0.0022]], grad_fn=<SoftmaxBackward0>)\n",
            "target =  tensor([0])\n",
            "loss =  tensor(2.1291, grad_fn=<NllLossBackward0>)\n",
            "\n",
            "Backward Propagation\n",
            "old linear 2 weight =  tensor([[ 0.,  0., -1.,  1.],\n",
            "        [ 0.,  1.,  1., -3.],\n",
            "        [ 1., -2., -3.,  3.]])\n",
            "old linear 1 weight =  tensor([[ 2.,  0.,  0.,  2.,  1., -2., -1.,  0.,  0., -1.,  1., -3., -1., -3.,\n",
            "         -2., -1.,  2., -3.],\n",
            "        [-2.,  3.,  2., -2., -3.,  0.,  2.,  2.,  1.,  3., -3., -3.,  0.,  1.,\n",
            "          3.,  2.,  3., -2.],\n",
            "        [-3.,  3.,  0.,  1., -3.,  1.,  2.,  3., -2.,  2.,  1.,  0.,  3.,  0.,\n",
            "         -3., -3., -1., -3.],\n",
            "        [-2., -1.,  2.,  0., -2.,  0.,  0.,  2., -3.,  3.,  3.,  2.,  3., -3.,\n",
            "         -3.,  0., -1.,  0.]])\n",
            "old convolution weight =  tensor([[[[ 1., -1.,  2.],\n",
            "          [-1., -2.,  0.],\n",
            "          [ 0.,  2., -1.]]],\n",
            "\n",
            "\n",
            "        [[[-2.,  1., -1.],\n",
            "          [ 2.,  0.,  0.],\n",
            "          [ 0.,  1., -1.]]]])\n",
            "\n",
            "new linear 2 grad =  tensor([[-6.1674e+00, -3.5242e+00, -8.8106e-01, -0.0000e+00],\n",
            "        [ 6.1521e+00,  3.5155e+00,  8.7888e-01,  0.0000e+00],\n",
            "        [ 1.5250e-02,  8.7141e-03,  2.1785e-03,  0.0000e+00]])\n",
            "new linear 2 bias grad =  tensor([[-0.8811,  0.8789,  0.0022]])\n",
            "new linear 1 grad =  tensor([[0.0000e+00, 4.3570e-03, 0.0000e+00, 0.0000e+00, 8.7141e-03, 0.0000e+00,\n",
            "         2.1785e-03, 0.0000e+00, 6.5356e-03, 0.0000e+00, 1.0893e-02, 0.0000e+00,\n",
            "         1.0893e-02, 0.0000e+00, 6.5356e-03, 0.0000e+00, 8.7141e-03, 0.0000e+00],\n",
            "        [0.0000e+00, 1.7490e+00, 0.0000e+00, 0.0000e+00, 3.4981e+00, 0.0000e+00,\n",
            "         8.7452e-01, 0.0000e+00, 2.6236e+00, 0.0000e+00, 4.3726e+00, 0.0000e+00,\n",
            "         4.3726e+00, 0.0000e+00, 2.6236e+00, 0.0000e+00, 3.4981e+00, 0.0000e+00],\n",
            "        [0.0000e+00, 3.5068e+00, 0.0000e+00, 0.0000e+00, 7.0136e+00, 0.0000e+00,\n",
            "         1.7534e+00, 0.0000e+00, 5.2602e+00, 0.0000e+00, 8.7670e+00, 0.0000e+00,\n",
            "         8.7670e+00, 0.0000e+00, 5.2602e+00, 0.0000e+00, 7.0136e+00, 0.0000e+00],\n",
            "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]])\n",
            "new linear 1 bias grad =  tensor([[0.0022, 0.8745, 1.7534, 0.0000]])\n",
            "new convolution grad =  tensor([[[[-1.3144e+01,  2.6279e+00,  4.3569e-03],\n",
            "          [ 2.1019e+01, -7.8794e+00,  7.8838e+00],\n",
            "          [-2.1019e+01,  5.2515e+00,  7.8816e+00]]],\n",
            "\n",
            "\n",
            "        [[[ 2.6345e+00,  3.4915e+00, -7.8990e+00],\n",
            "          [ 2.6236e+00, -4.3900e+00,  6.1565e+00],\n",
            "          [ 6.1260e+00,  7.0310e+00,  8.7452e-01]]]])\n",
            "\n",
            "new linear 2 weight =  Parameter containing:\n",
            "tensor([[ 0.0617,  0.0352, -0.9912,  1.0000],\n",
            "        [-0.0615,  0.9648,  0.9912, -3.0000],\n",
            "        [ 0.9998, -2.0001, -3.0000,  3.0000]], requires_grad=True)\n",
            "new linear 2 bias =  Parameter containing:\n",
            "tensor([[ 3.0088, -1.0088,  2.0000]], requires_grad=True)\n",
            "new linear 1 weight =  Parameter containing:\n",
            "tensor([[ 2.0000e+00, -4.3570e-05,  0.0000e+00,  2.0000e+00,  9.9991e-01,\n",
            "         -2.0000e+00, -1.0000e+00,  0.0000e+00, -6.5356e-05, -1.0000e+00,\n",
            "          9.9989e-01, -3.0000e+00, -1.0001e+00, -3.0000e+00, -2.0001e+00,\n",
            "         -1.0000e+00,  1.9999e+00, -3.0000e+00],\n",
            "        [-2.0000e+00,  2.9825e+00,  2.0000e+00, -2.0000e+00, -3.0350e+00,\n",
            "          0.0000e+00,  1.9913e+00,  2.0000e+00,  9.7376e-01,  3.0000e+00,\n",
            "         -3.0437e+00, -3.0000e+00, -4.3726e-02,  1.0000e+00,  2.9738e+00,\n",
            "          2.0000e+00,  2.9650e+00, -2.0000e+00],\n",
            "        [-3.0000e+00,  2.9649e+00,  0.0000e+00,  1.0000e+00, -3.0701e+00,\n",
            "          1.0000e+00,  1.9825e+00,  3.0000e+00, -2.0526e+00,  2.0000e+00,\n",
            "          9.1233e-01,  0.0000e+00,  2.9123e+00,  0.0000e+00, -3.0526e+00,\n",
            "         -3.0000e+00, -1.0701e+00, -3.0000e+00],\n",
            "        [-2.0000e+00, -1.0000e+00,  2.0000e+00,  0.0000e+00, -2.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00,  2.0000e+00, -3.0000e+00,  3.0000e+00,\n",
            "          3.0000e+00,  2.0000e+00,  3.0000e+00, -3.0000e+00, -3.0000e+00,\n",
            "          0.0000e+00, -1.0000e+00,  0.0000e+00]], requires_grad=True)\n",
            "new linear 1 bias =  Parameter containing:\n",
            "tensor([[ 2.0000, -1.0087,  3.9825,  0.0000]], requires_grad=True)\n",
            "new convolution weight =  Parameter containing:\n",
            "tensor([[[[ 1.1314, -1.0263,  2.0000],\n",
            "          [-1.2102, -1.9212, -0.0788],\n",
            "          [ 0.2102,  1.9475, -1.0788]]],\n",
            "\n",
            "\n",
            "        [[[-2.0263,  0.9651, -0.9210],\n",
            "          [ 1.9738,  0.0439, -0.0616],\n",
            "          [-0.0613,  0.9297, -1.0087]]]], requires_grad=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:23: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        }
      ]
    }
  ]
}